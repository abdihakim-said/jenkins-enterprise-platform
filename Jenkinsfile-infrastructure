#!/usr/bin/env groovy
/*
 * Infrastructure Deployment Pipeline - Aligned with Current Terraform Setup
 * Uses existing modules: vpc, security, iam, efs, alb, blue-green-deployment, cloudwatch, inspector, cost-optimized-observability, cost-optimization
 */

pipeline {
    agent any
    
    parameters {
        choice(name: 'ENVIRONMENT', choices: ['dev', 'staging', 'production'], description: 'Target environment')
        booleanParam(name: 'SKIP_SECURITY_SCAN', defaultValue: false, description: 'Skip security scanning')
        booleanParam(name: 'DRY_RUN', defaultValue: false, description: 'Plan only, do not apply')
        booleanParam(name: 'FORCE_NEW_AMI', defaultValue: false, description: 'Force use latest AMI even if no changes')
        choice(name: 'DEPLOYMENT_STRATEGY', choices: ['standard', 'blue-green'], description: 'Deployment strategy')
        booleanParam(name: 'ENABLE_BACKUP', defaultValue: false, description: 'Enable automated backup')
    }
    
    environment {
        AWS_DEFAULT_REGION = 'us-east-1'
        TF_IN_AUTOMATION = 'true'
        TF_INPUT = 'false'
        TF_VAR_environment = "${params.ENVIRONMENT}"
        DR_REGION = 'us-west-2'
    }
    
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 1, unit: 'HOURS')
        timestamps()
    }
    
    stages {
        stage('ðŸ”’ Infrastructure Security Scan') {
            when { equals expected: false, actual: params.SKIP_SECURITY_SCAN }
            parallel {
                stage('TFSec Scan') {
                    steps {
                        sh '''
                            echo "ðŸ” Running TFSec security scan..."
                            docker run --rm -v $PWD:/src aquasec/tfsec /src \
                                --format json --out tfsec-results.json --soft-fail
                            
                            CRITICAL=$(jq '[.results[]? | select(.severity=="CRITICAL")] | length' tfsec-results.json)
                            if [ "$CRITICAL" -gt "0" ]; then
                                echo "âŒ Critical security issues: $CRITICAL"
                                exit 1
                            fi
                            echo "âœ… TFSec scan passed"
                        '''
                    }
                }
                
                stage('Checkov Scan') {
                    steps {
                        sh '''
                            echo "ðŸ” Running Checkov IaC scan..."
                            docker run --rm -v $PWD:/tf bridgecrew/checkov \
                                -d /tf --framework terraform \
                                --output json --output-file-path /tf/checkov-results.json --soft-fail
                            echo "âœ… Checkov scan completed"
                        '''
                    }
                }
                
                stage('Secrets Scan') {
                    steps {
                        sh '''
                            echo "ðŸ” Scanning for secrets..."
                            docker run --rm -v $PWD:/path zricethezav/gitleaks detect \
                                --source="/path" --report-format json \
                                --report-path gitleaks-results.json --exit-code 0
                            
                            if [ -s gitleaks-results.json ]; then
                                SECRETS=$(jq length gitleaks-results.json)
                                if [ "$SECRETS" -gt "0" ]; then
                                    echo "âŒ Secrets detected: $SECRETS"
                                    exit 1
                                fi
                            fi
                            echo "âœ… No secrets detected"
                        '''
                    }
                }
            }
        }
        
        stage('ðŸ“‹ Check Latest Golden AMI') {
            steps {
                script {
                    sh '''
                        echo "ðŸ“‹ Checking for latest Golden AMI..."
                        
                        # Get latest AMI matching your naming pattern
                        LATEST_AMI=$(aws ec2 describe-images \
                            --owners self \
                            --filters "Name=name,Values=jenkins-golden-ami-*" "Name=state,Values=available" \
                            --query 'Images | sort_by(@, &CreationDate) | [-1].ImageId' \
                            --output text)
                        
                        if [ "$LATEST_AMI" = "None" ] || [ -z "$LATEST_AMI" ]; then
                            echo "âŒ No Golden AMI found! Run AMI Factory pipeline first."
                            exit 1
                        fi
                        
                        echo "LATEST_AMI_ID=$LATEST_AMI" > ami.env
                        
                        # Get AMI details
                        aws ec2 describe-images --image-ids $LATEST_AMI \
                            --query 'Images[0].{Name:Name,CreationDate:CreationDate,Tags:Tags}' \
                            --output table
                        
                        echo "âœ… Using Golden AMI: $LATEST_AMI"
                    '''
                }
            }
        }
        
        stage('ðŸ—ï¸ Terraform Plan') {
            steps {
                script {
                    sh '''
                        echo "ðŸ—ï¸ Planning infrastructure for ${ENVIRONMENT}..."
                        
                        # Assume deployment role for infrastructure operations
                        DEPLOYMENT_ROLE_ARN="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/${ENVIRONMENT}-jenkins-enterprise-platform-deployment-role"
                        
                        # Get temporary credentials for deployment role
                        TEMP_CREDS=$(aws sts assume-role --role-arn $DEPLOYMENT_ROLE_ARN --role-session-name "jenkins-deployment-${BUILD_NUMBER}")
                        
                        export AWS_ACCESS_KEY_ID=$(echo $TEMP_CREDS | jq -r '.Credentials.AccessKeyId')
                        export AWS_SECRET_ACCESS_KEY=$(echo $TEMP_CREDS | jq -r '.Credentials.SecretAccessKey')
                        export AWS_SESSION_TOKEN=$(echo $TEMP_CREDS | jq -r '.Credentials.SessionToken')
                        
                        # Initialize Terraform with environment-specific backend (reconfigure to handle changes)
                        terraform init -backend-config="key=jenkins/${ENVIRONMENT}/terraform.tfstate" -reconfigure
                        terraform validate
                        
                        # Check for state drift and import existing resources if needed
                        echo "ðŸ” Checking for state drift before planning..."
                        
                        # Function to import S3 bucket if it exists but not in state
                        import_s3_if_needed() {
                            local bucket_name=$1
                            local terraform_resource=$2
                            
                            # Check if bucket exists in AWS
                            if aws s3api head-bucket --bucket "$bucket_name" 2>/dev/null; then
                                echo "ðŸ“¦ Bucket $bucket_name exists in AWS"
                                
                                # Check if it's in Terraform state
                                if ! terraform state show "$terraform_resource" 2>/dev/null; then
                                    echo "âš ï¸  Bucket not in state, importing: $terraform_resource"
                                    terraform import "$terraform_resource" "$bucket_name" || echo "Import failed or already exists"
                                else
                                    echo "âœ… Bucket already in state: $terraform_resource"
                                fi
                            fi
                        }
                        
                        # Get existing bucket names
                        ALB_BUCKET=$(aws s3 ls | grep "${ENVIRONMENT}-jenkins-alb-logs" | awk '{print $3}' | head -1)
                        CLOUDTRAIL_BUCKET=$(aws s3 ls | grep "${ENVIRONMENT}-jenkins-cloudtrail" | awk '{print $3}' | head -1)
                        
                        # Import buckets if found
                        if [ -n "$ALB_BUCKET" ]; then
                            import_s3_if_needed "$ALB_BUCKET" "module.alb.aws_s3_bucket.alb_logs"
                        fi
                        if [ -n "$CLOUDTRAIL_BUCKET" ]; then
                            import_s3_if_needed "$CLOUDTRAIL_BUCKET" "module.security_automation.aws_s3_bucket.cloudtrail"
                        fi
                        
                        # Plan deployment with environment-specific variables
                        terraform plan -var-file="environments/${ENVIRONMENT}/terraform.tfvars" -out=${ENVIRONMENT}-deploy.tfplan
                        
                        # Generate plan analysis
                        terraform show -json ${ENVIRONMENT}-deploy.tfplan > ${ENVIRONMENT}-plan.json
                        
                        echo "âœ… Terraform plan completed"
                    '''
                }
            }
        }
        
        stage('ðŸ“Š Plan Analysis & Approval') {
            steps {
                script {
                    sh '''
                        echo "ðŸ“Š Analyzing Terraform plan..."
                        
                        # Extract plan statistics
                        RESOURCES_TO_ADD=$(jq '[.planned_values.root_module.resources[]? | select(.mode=="managed")] | length' ${ENVIRONMENT}-plan.json)
                        RESOURCES_TO_CHANGE=$(jq '[.resource_changes[]? | select(.change.actions[] | contains("update"))] | length' ${ENVIRONMENT}-plan.json)
                        RESOURCES_TO_DESTROY=$(jq '[.resource_changes[]? | select(.change.actions[] | contains("delete"))] | length' ${ENVIRONMENT}-plan.json)
                        
                        echo "=== Plan Summary ==="
                        echo "Resources to add: $RESOURCES_TO_ADD"
                        echo "Resources to change: $RESOURCES_TO_CHANGE"  
                        echo "Resources to destroy: $RESOURCES_TO_DESTROY"
                        echo "===================="
                        
                        # Check for destructive changes in production
                        if [ "${ENVIRONMENT}" = "production" ] && [ "$RESOURCES_TO_DESTROY" -gt "0" ]; then
                            echo "âš ï¸ WARNING: Production deployment will destroy $RESOURCES_TO_DESTROY resources"
                        fi
                    '''
                    
                    // Production requires manual approval
                    if (params.ENVIRONMENT == 'production' && !params.DRY_RUN) {
                        timeout(time: 10, unit: 'MINUTES') {
                            input message: "Deploy to Production?", 
                                  ok: 'Deploy',
                                  submitterParameter: 'APPROVER'
                        }
                    }
                }
            }
        }
        
        stage('ðŸš€ Deploy Infrastructure') {
            when { equals expected: false, actual: params.DRY_RUN }
            steps {
                script {
                    sh '''
                        echo "ðŸš€ Deploying infrastructure to ${ENVIRONMENT}..."
                        
                        # Assume deployment role for infrastructure operations
                        DEPLOYMENT_ROLE_ARN="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/${ENVIRONMENT}-jenkins-enterprise-platform-deployment-role"
                        
                        # Get temporary credentials for deployment role
                        TEMP_CREDS=$(aws sts assume-role --role-arn $DEPLOYMENT_ROLE_ARN --role-session-name "jenkins-deployment-${BUILD_NUMBER}")
                        
                        export AWS_ACCESS_KEY_ID=$(echo $TEMP_CREDS | jq -r '.Credentials.AccessKeyId')
                        export AWS_SECRET_ACCESS_KEY=$(echo $TEMP_CREDS | jq -r '.Credentials.SecretAccessKey')
                        export AWS_SESSION_TOKEN=$(echo $TEMP_CREDS | jq -r '.Credentials.SessionToken')
                        
                        # Import existing S3 buckets if they exist (before apply)
                        echo "ðŸ” Checking for existing S3 buckets before apply..."
                        
                        # Check and import ALB logs bucket
                        ALB_BUCKET=$(aws s3 ls | grep "${ENVIRONMENT}-jenkins-alb-logs" | awk '{print $3}' | head -1)
                        if [ -n "$ALB_BUCKET" ]; then
                            echo "ðŸ“¦ Found existing ALB bucket: $ALB_BUCKET"
                            if ! terraform state show module.alb.aws_s3_bucket.alb_logs >/dev/null 2>&1; then
                                echo "â¬‡ï¸ Importing ALB bucket to Terraform state..."
                                terraform import module.alb.aws_s3_bucket.alb_logs "$ALB_BUCKET" || echo "âš ï¸ ALB bucket import failed or already exists"
                            fi
                        fi
                        
                        # Check and import CloudTrail bucket
                        CLOUDTRAIL_BUCKET=$(aws s3 ls | grep "${ENVIRONMENT}-jenkins-cloudtrail" | awk '{print $3}' | head -1)
                        if [ -n "$CLOUDTRAIL_BUCKET" ]; then
                            echo "ðŸ“¦ Found existing CloudTrail bucket: $CLOUDTRAIL_BUCKET"
                            if ! terraform state show module.security_automation.aws_s3_bucket.cloudtrail >/dev/null 2>&1; then
                                echo "â¬‡ï¸ Importing CloudTrail bucket to Terraform state..."
                                terraform import module.security_automation.aws_s3_bucket.cloudtrail "$CLOUDTRAIL_BUCKET" || echo "âš ï¸ CloudTrail bucket import failed or already exists"
                            fi
                        fi
                        
                        # Check and import Cost Optimization bucket
                        COST_BUCKET=$(aws s3 ls | grep "${ENVIRONMENT}-jenkins-cost-optimization" | awk '{print $3}' | head -1)
                        if [ -n "$COST_BUCKET" ]; then
                            echo "ðŸ“¦ Found existing Cost Optimization bucket: $COST_BUCKET"
                            if ! terraform state show module.cost_optimization.aws_s3_bucket.cost_reports >/dev/null 2>&1; then
                                echo "â¬‡ï¸ Importing Cost Optimization bucket to Terraform state..."
                                terraform import module.cost_optimization.aws_s3_bucket.cost_reports "$COST_BUCKET" || echo "âš ï¸ Cost Optimization bucket import failed or already exists"
                            fi
                        fi
                        
                        # Apply the plan from root directory
                        terraform apply -auto-approve ${ENVIRONMENT}-deploy.tfplan
                        
                        # Generate outputs
                        terraform output -json > ${ENVIRONMENT}-outputs.json
                        
                        # Verify outputs file was created
                        if [ -f "${ENVIRONMENT}-outputs.json" ]; then
                            echo "âœ… Outputs file created: ${ENVIRONMENT}-outputs.json"
                            ls -la ${ENVIRONMENT}-outputs.json
                        else
                            echo "âŒ Failed to create outputs file"
                            exit 1
                        fi
                        
                        echo "âœ… Infrastructure deployed successfully"
                    '''
                }
            }
        }
        
        stage('âœ… Post-Deployment Validation') {
            when { equals expected: false, actual: params.DRY_RUN }
            parallel {
                stage('Health Check') {
                    steps {
                        script {
                            sh '''
                                echo "âœ… Running health checks..."
                                
                                # Get Jenkins URL from outputs (matches your output format)
                                JENKINS_URL=$(jq -r '.jenkins_url.value' ${ENVIRONMENT}-outputs.json)
                                
                                if [ "$JENKINS_URL" != "null" ]; then
                                    echo "Jenkins URL: $JENKINS_URL"
                                    
                                    # Wait for Jenkins to be accessible
                                    echo "Waiting for Jenkins to be ready..."
                                    timeout 600 bash -c "until curl -s $JENKINS_URL/login; do sleep 15; done"
                                    
                                    # Validate Jenkins login page
                                    if curl -s $JENKINS_URL/login | grep -q "Jenkins"; then
                                        echo "âœ… Jenkins is accessible at: $JENKINS_URL"
                                    else
                                        echo "âŒ Jenkins health check failed"
                                        exit 1
                                    fi
                                else
                                    echo "âŒ No Jenkins URL found in outputs"
                                    exit 1
                                fi
                            '''
                        }
                    }
                }
                
                stage('Infrastructure Validation') {
                    steps {
                        script {
                            sh '''
                                echo "ðŸ” Validating infrastructure components..."
                                
                                # Validate VPC
                                VPC_ID=$(jq -r '.vpc_id.value' ${ENVIRONMENT}-outputs.json)
                                aws ec2 describe-vpcs --vpc-ids $VPC_ID --query 'Vpcs[0].State' --output text
                                
                                # Validate Auto Scaling Group
                                ASG_NAME=$(jq -r '.jenkins_auto_scaling_group_name.value' ${ENVIRONMENT}-outputs.json)
                                aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names $ASG_NAME \
                                    --query 'AutoScalingGroups[0].{DesiredCapacity:DesiredCapacity,Instances:length(Instances)}' \
                                    --output table
                                
                                # Validate EFS
                                EFS_ID=$(jq -r '.efs_file_system_id.value' ${ENVIRONMENT}-outputs.json)
                                aws efs describe-file-systems --file-system-id $EFS_ID \
                                    --query 'FileSystems[0].LifeCycleState' --output text
                                
                                echo "âœ… Infrastructure validation completed"
                            '''
                        }
                    }
                }
                
                stage('Security Validation') {
                    steps {
                        script {
                            sh '''
                                echo "ðŸ”’ Running security validation..."
                                
                                JENKINS_URL=$(jq -r '.jenkins_url.value' ${ENVIRONMENT}-outputs.json)
                                
                                # Check security headers
                                echo "Checking security headers..."
                                curl -I $JENKINS_URL 2>/dev/null | grep -i "x-content-type-options" && echo "âœ… Security headers present" || echo "âš ï¸ Missing security headers"
                                
                                # Validate security groups
                                SG_ID=$(jq -r '.jenkins_security_group_id.value' ${ENVIRONMENT}-outputs.json)
                                aws ec2 describe-security-groups --group-ids $SG_ID \
                                    --query 'SecurityGroups[0].GroupName' --output text
                                
                                echo "âœ… Security validation completed"
                            '''
                        }
                    }
                }
            }
        }
        
        stage('ðŸ“‹ Generate Deployment Report') {
            when { equals expected: false, actual: params.DRY_RUN }
            steps {
                script {
                    sh '''
                        echo "ðŸ“‹ Generating deployment report..."
                        
                        # Load AMI info
                        source ami.env
                        
                        # Create comprehensive deployment report
                        cat > deployment-report-${ENVIRONMENT}.json << EOF
{
    "deployment": {
        "environment": "${ENVIRONMENT}",
        "build_number": "${BUILD_NUMBER}",
        "ami_id": "$LATEST_AMI_ID",
        "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
        "approver": "${APPROVER:-automated}",
        "dry_run": ${DRY_RUN}
    },
    "security": {
        "tfsec_scan": $([ "${SKIP_SECURITY_SCAN}" = "false" ] && echo "true" || echo "false"),
        "checkov_scan": $([ "${SKIP_SECURITY_SCAN}" = "false" ] && echo "true" || echo "false"),
        "secrets_scan": $([ "${SKIP_SECURITY_SCAN}" = "false" ] && echo "true" || echo "false")
    },
    "infrastructure": $(cat ${ENVIRONMENT}-outputs.json),
    "validation": {
        "health_check": "passed",
        "security_check": "passed",
        "infrastructure_check": "passed"
    }
}
EOF
                        
                        # Store report in S3 (using your existing backup bucket pattern)
                        aws s3 cp deployment-report-${ENVIRONMENT}.json s3://jenkins-enterprise-backup/deployment-reports/$(date +%Y%m%d)/
                        
                        echo "âœ… Deployment report generated and stored"
                    '''
                }
            }
        }
    }
    
    post {
        always {
            archiveArtifacts artifacts: '''
                **/*-results.json,
                **/*-plan.json,
                **/*-outputs.json,
                deployment-report-*.json,
                ami.env
            ''', allowEmptyArchive: true
        }
        
        success {
            script {
                if (!params.DRY_RUN) {
                    def jenkinsUrl = sh(script: 'jq -r ".jenkins_url.value" ${ENVIRONMENT}-outputs.json', returnStdout: true).trim()
                    def amiId = sh(script: 'source ami.env && echo $LATEST_AMI_ID', returnStdout: true).trim()
                    
                    echo """
ðŸŽ‰ Infrastructure Deployment Successful!
========================================
Environment: ${params.ENVIRONMENT}
Jenkins URL: ${jenkinsUrl}
AMI ID: ${amiId}
Build: ${BUILD_NUMBER}
Duration: ${currentBuild.durationString}

Infrastructure is ready! ðŸš€

Connection Info:
- URL: ${jenkinsUrl}
- Admin Password: aws ssm get-parameter --name '/jenkins/${params.ENVIRONMENT}/admin-password' --with-decryption --query 'Parameter.Value' --output text --region us-east-1
"""
                } else {
                    echo "âœ… Dry run completed successfully - no resources deployed"
                }
            }
        }
        
        failure {
            script {
                echo """
âŒ Infrastructure Deployment Failed!
===================================
Environment: ${params.ENVIRONMENT}
Build: ${BUILD_NUMBER}
Duration: ${currentBuild.durationString}

Check logs for details.
"""
            }
        }
    }
}
